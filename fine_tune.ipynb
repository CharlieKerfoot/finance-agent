{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyO/cDifEZWXB0uclbah4e64",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CharlieKerfoot/finance-agent/blob/main/fine_tune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0CJgsxG6UmG"
      },
      "outputs": [],
      "source": [
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install weave wandb\n",
        "!pip install --no-deps xformer trl peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "CJlysm48igCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import weave\n",
        "\n",
        "run = wandb.init(\n",
        "    project=\"Arb-Agent-v1\",\n",
        "    name=\"llama3-8b-lora-run3\",\n",
        "    job_type=\"training\",\n",
        ")"
      ],
      "metadata": {
        "id": "x-4RVGn1mZwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "max_seq_length = 8192\n",
        "dtype = None\n",
        "load_in_4bit = True\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    dtype=dtype,\n",
        "    load_in_4bit=load_in_4bit\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407\n",
        ")"
      ],
      "metadata": {
        "id": "hVrnunxZ86sd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"ckerf/arb-instruct-v1\", split=\"train\")\n",
        "\n",
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "def format_prompts(examples):\n",
        "    instructions = examples[\"instruction\"]\n",
        "    inputs = examples[\"input\"]\n",
        "    outputs = examples[\"output\"]\n",
        "    texts = []\n",
        "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "      text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "      texts.append(text)\n",
        "    return {\n",
        "      \"text\": texts,\n",
        "    }\n",
        "\n",
        "dataset = dataset.map(format_prompts, batched=True)\n"
      ],
      "metadata": {
        "id": "agRjfMFG0WLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = 8192,\n",
        "    dataset_num_proc = 8,\n",
        "    packing = True,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 8,\n",
        "        gradient_accumulation_steps = 2,\n",
        "        num_train_epochs=1,\n",
        "        warmup_steps = 10,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = False,\n",
        "        bf16 = True,\n",
        "        logging_steps = 25,\n",
        "        logging_strategy = \"steps\",\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        report_to = \"wandb\",\n",
        "        run_name = \"llama3-8b-lora-run2\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "BVQV3-21QgM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "import random\n",
        "samples = random.sample(list(dataset), 5)\n",
        "\n",
        "for sample in samples:\n",
        "    inputs = tokenizer(\n",
        "        [\n",
        "            alpaca_prompt.format(\n",
        "                sample[\"instruction\"],\n",
        "                sample[\"input\"],\n",
        "                \"\",\n",
        "            )\n",
        "        ], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "    outputs = model.generate(**inputs, max_new_tokens = 512, use_cache = True)\n",
        "\n",
        "    print(\"\\n\" + \"-\" * 50 + \"\\n\")\n",
        "    print(f\"Q: {sample['instruction']}\")\n",
        "    print(f\"A:\\n{tokenizer.batch_decode(outputs)[0].split('### Response:')[1]}\")\n",
        "    print(f\"Ground Truth:\\n{sample['output']}\")"
      ],
      "metadata": {
        "id": "Dk6LNag1bmrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "b5UV0dZYdPnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(\"ckerf/arbagent-llama3-8b-lora\")\n",
        "tokenizer.push_to_hub(\"ckerf/arbagent-llama3-8b-lora\")"
      ],
      "metadata": {
        "id": "jztS6L-td8DH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}